{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import kagglehub\n",
    "\n",
    "# Get Dataset\n",
    "dataset_path = kagglehub.dataset_download(\"shaz13/real-world-documents-collections\")"
   ],
   "id": "fbc121e30a2defb3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "dataset_path += '\\\\docs-sm'",
   "id": "a042b4654cddcdd7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split, Subset\n",
    "\n",
    "# Define transforms (resize, normalize, etc.)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((320, 320)),\n",
    "    transforms.RandomHorizontalFlip(0.25),\n",
    "    transforms.RandomRotation(5),\n",
    "    transforms.RandomPerspective(distortion_scale=0.25, p=0.25),\n",
    "    transforms.ColorJitter(brightness=0.25, contrast=0.25),\n",
    "    transforms.ToTensor(),\n",
    "\n",
    "    # Normalization Factor for Resnet50\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Transformations that don't really normalize or alter the data that much.\n",
    "# They just serve to put the datain to the proper format for the models\n",
    "raw_transform = transforms.Compose([\n",
    "    transforms.Resize((320, 320)),  # Ensures all images are same size\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Two datasets with same file paths\n",
    "full_dataset_with_aug = datasets.ImageFolder(root=dataset_path, transform=transform)\n",
    "full_dataset_raw = datasets.ImageFolder(root=dataset_path, transform=raw_transform)\n",
    "\n",
    "\n",
    "# Create DataLoader\n",
    "# Store split sizes\n",
    "train_len = int(0.9 * len(full_dataset_with_aug))\n",
    "test_len = len(full_dataset_with_aug) - train_len\n",
    "seed = 42\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "train_set, test_set_with_aug = random_split(full_dataset_with_aug, [train_len, test_len])\n",
    "\n",
    "test_indices = test_set_with_aug.indices\n",
    "test_set_raw = Subset(full_dataset_raw, test_indices)\n",
    "\n",
    "train_dataloader = DataLoader(train_set, batch_size=8, shuffle=True)\n",
    "test_dataloader = DataLoader(test_set_with_aug, batch_size=8, shuffle=True)\n",
    "test_raw_dataloader = DataLoader(test_set_raw, batch_size=8, shuffle=True)"
   ],
   "id": "ce6b8f21dc9dc1c7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to unnormalize\n",
    "def unnormalize(img_tensor):\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "    return img_tensor * std + mean\n",
    "\n",
    "# Get a batch of images\n",
    "images, labels = next(iter(test_raw_dataloader))\n",
    "\n",
    "images = unnormalize(images)\n",
    "\n",
    "num_images = 9  # 3x3 grid\n",
    "\n",
    "rows = cols = int(num_images ** 0.5)\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(cols * 4, rows * 4))  # 4x4 inches per image\n",
    "\n",
    "# Plot each image\n",
    "for i in range(num_images):\n",
    "    ax = axes[i // cols, i % cols]\n",
    "    img = images[i].permute(1, 2, 0).clamp(0, 1).numpy()  # Clamp to [0, 1] for safe display\n",
    "    ax.imshow(img)\n",
    "    ax.axis('off')\n",
    "    ax.set_title(f\"Label: {labels[i]}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "e16e9f1033da5438",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "class CNNClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=1):\n",
    "        super().__init__()\n",
    "\n",
    "        # CNN Stack\n",
    "        self.convolution_stack = nn.Sequential(\n",
    "\n",
    "            nn.Conv2d(3, 32, kernel_size=7, stride=2, padding=3),   # -> [B, 32, 112, 112]\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),  # -> [B, 64, 112, 112]\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),                  # -> [B, 64, 56, 56]\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1), # -> [B, 128, 56, 56]\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=2, dilation=2),  # -> [B, 128, 56, 56]\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=4, dilation=4),  # -> [B, 256, 56, 56]\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Conv2d(256, 384, kernel_size=3, stride=1, padding=8, dilation=8),  # -> [B, 384, 56, 56]\n",
    "            nn.BatchNorm2d(384),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.AdaptiveAvgPool2d((1,1))  # Final output shape: [B, 384, 1, 1]\n",
    "        )\n",
    "\n",
    "        # Classifier Head\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(384, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.convolution_stack(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ],
   "id": "c37bb3390b18891e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch.optim as optim\n",
    "from datasets import tqdm\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "def train(model, save_path):\n",
    "\n",
    "    # Training setup\n",
    "    criterion = nn.CrossEntropyLoss() # Cross Entropy Loss for classification :)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "    model.to(\"cuda\")\n",
    "\n",
    "    for epoch in range(40):\n",
    "        train_loss_total = 0\n",
    "        for inputs, labels in tqdm(train_dataloader):\n",
    "            inputs = inputs.to(\"cuda\")\n",
    "            labels = labels.to(\"cuda\")\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss_total += loss.item()\n",
    "\n",
    "        test_loss_average, correct, total, all_preds, all_labels = eval(model, test_dataloader, criterion)\n",
    "\n",
    "        precision_macro = precision_score(all_labels, all_preds, average='macro')\n",
    "        recall_macro = recall_score(all_labels, all_preds, average='macro')\n",
    "        f1_macro = f1_score(all_labels, all_preds, average='macro')\n",
    "\n",
    "        train_loss_average = train_loss_total / len(train_dataloader)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}, Train Loss: {train_loss_average:.4f}\")\n",
    "        print(f\"Epoch {epoch+1}, Test Loss: {test_loss_average:.4f}\")\n",
    "        print(f\"Test Accuracy: {correct / total:.2%}\")\n",
    "        print(f\"Precision: {precision_macro:.4f}\\t Recall: {recall_macro:.4f}\\t F1: {f1_macro:.4f}\")\n",
    "\n",
    "        torch.save(model, f\"{save_path}\\\\Classifier-Epoch-{epoch + 1}.pt\")\n",
    "\n",
    "def eval(model, dataloader, criterion):\n",
    "    test_loss_total = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(dataloader):\n",
    "            inputs = inputs.to(\"cuda\")\n",
    "            labels = labels.to(\"cuda\")\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            test_loss_total += loss.item()\n",
    "\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    return  test_loss_total / len(dataloader), correct, total, all_preds, all_labels"
   ],
   "id": "9b2bfb82a6ecfc85",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "def evaluate_and_plot(model):\n",
    "    loss, correct, total, all_preds, all_labels = eval(model, test_raw_dataloader, nn.CrossEntropyLoss())\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "    # Get class names (assuming ImageFolder was used)\n",
    "    class_names = full_dataset_raw.classes  # <-- adjust this to match your dataset\n",
    "\n",
    "    # Plot\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    disp.plot(ax=ax, cmap='Blues', xticks_rotation=45)\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "id": "d2214bf31858a48",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-17T01:17:43.710362Z",
     "start_time": "2025-08-17T01:17:43.609363Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 1,
   "source": [
    "import numpy as np\n",
    "\n",
    "# We want our models to be roughly the same size, that way when we compare the performance of the architecture\n",
    "# we are concentrating on the effect that the architecture had, not the model size.\n",
    "\n",
    "def model_size(model):\n",
    "    model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    return sum([np.prod(p.size()) for p in model_parameters])"
   ],
   "id": "8de391ffd15effe7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model = CNNClassifier(num_classes=len(full_dataset_raw.classes))\n",
    "print(model_size(model))\n",
    "\n",
    "train(model, \"CNNClassifier\")\n",
    "evaluate_and_plot(model)"
   ],
   "id": "22ade997a800907c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch.nn as nn\n",
    "from torchvision.models import resnet50\n",
    "\n",
    "model = resnet50(pretrained=True)\n",
    "\n",
    "# Replace the last layer as Resnet doesn't inherently have a classifier head\n",
    "num_features = model.fc.in_features\n",
    "model.fc = nn.Linear(num_features, 16)\n",
    "print(model_size(model))\n",
    "\n",
    "train(model, \"Resnet50\")\n",
    "evaluate_and_plot(model)\n"
   ],
   "id": "779939c89747574e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "from vit_pytorch import ViT\n",
    "\n",
    "vit = ViT(\n",
    "    image_size = 320,\n",
    "    patch_size = 8,\n",
    "    num_classes = len(full_dataset_raw.classes),\n",
    "    dim = 128,\n",
    "    depth = 8,\n",
    "    heads = 4,\n",
    "    mlp_dim = 128 * 4,\n",
    "    dropout = 0.1,\n",
    "    emb_dropout = 0.1\n",
    ")\n",
    "\n",
    "print(model_size(model))\n",
    "\n",
    "train(model, \"ViT\")\n",
    "evaluate_and_plot(model)"
   ],
   "id": "e01272bd6f388c11"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-17T01:23:06.361358Z",
     "start_time": "2025-08-17T01:23:02.144336Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'full_dataset_raw' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 11\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mvit_pytorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mnest\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m NesT\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mnn\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mnn\u001B[39;00m\n\u001B[0;32m      4\u001B[0m nest \u001B[38;5;241m=\u001B[39m NesT(\n\u001B[0;32m      5\u001B[0m     image_size \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m320\u001B[39m,\n\u001B[0;32m      6\u001B[0m     patch_size \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m8\u001B[39m,\n\u001B[0;32m      7\u001B[0m     dim \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m96\u001B[39m,\n\u001B[0;32m      8\u001B[0m     heads \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m4\u001B[39m,\n\u001B[0;32m      9\u001B[0m     num_hierarchies \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m3\u001B[39m,\n\u001B[0;32m     10\u001B[0m     block_repeats \u001B[38;5;241m=\u001B[39m (\u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m4\u001B[39m),  \u001B[38;5;66;03m# the number of transformer blocks at each hierarchy, starting from the bottom\u001B[39;00m\n\u001B[1;32m---> 11\u001B[0m     num_classes \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(\u001B[43mfull_dataset_raw\u001B[49m\u001B[38;5;241m.\u001B[39mclasses),\n\u001B[0;32m     12\u001B[0m     dropout \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.1\u001B[39m,\n\u001B[0;32m     13\u001B[0m )\n\u001B[0;32m     15\u001B[0m \u001B[38;5;28mprint\u001B[39m(model_size(nest))\n",
      "\u001B[1;31mNameError\u001B[0m: name 'full_dataset_raw' is not defined"
     ]
    }
   ],
   "execution_count": 2,
   "source": [
    "from vit_pytorch.nest import NesT\n",
    "import torch.nn as nn\n",
    "\n",
    "model = NesT(\n",
    "    image_size = 320,\n",
    "    patch_size = 8,\n",
    "    dim = 96,\n",
    "    heads = 4,\n",
    "    num_hierarchies = 3,\n",
    "    block_repeats = (2, 2, 4),  # the number of transformer blocks at each hierarchy, starting from the bottom\n",
    "    num_classes = len(full_dataset_raw.classes),\n",
    "    dropout = 0.1,\n",
    ")\n",
    "\n",
    "print(model_size(model))\n",
    "\n",
    "train(model, \"Nest\")\n",
    "evaluate_and_plot(model)"
   ],
   "id": "fdb40d24f9e19ea1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
